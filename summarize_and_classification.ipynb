{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "summarize_and_classification.ipynb",
      "authorship_tag": "ABX9TyO4bVbui9V8O76ZpkINeaPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/ML/blob/main/summarize_and_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91de1acc"
      },
      "source": [
        "%pip install azure-ai-formrecognizer openai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cac13c24"
      },
      "source": [
        "# Step 1: Parse document using Azure Document Intelligence\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from openai import OpenAI\n",
        "\n",
        "# Replace with your actual access token\n",
        "from google.colab import userdata\n",
        "DOCUMENTINTEL_KEY = userdata.get('DOCUMENTINTEL_KEY')\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive\n",
        "\n",
        "# Azure Document Intelligence setup\n",
        "endpoint = \"https://documentsclassifier.cognitiveservices.azure.com/\"\n",
        "key = DOCUMENTINTEL_KEY\n",
        "\n",
        "document_analysis_client = DocumentAnalysisClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(DOCUMENTINTEL_KEY)\n",
        ")\n",
        "\n",
        "# Analyze a document\n",
        "with open(f\"{data_dir}/RAG/data/10k/lyft_10k_2023.pdf\", \"rb\") as f:\n",
        "    # This is the name of the pre-trained model being used for the analysis. Azure Document Intelligence provides various pre-built models for different document types (like invoices, receipts, identity documents, etc.).\n",
        "    # In this case \"prebuilt-document\" is a general-purpose model that can extract text, layout\n",
        "    # and other key information from various types of documents.\n",
        "    poller = document_analysis_client.begin_analyze_document(\"prebuilt-document\", document=f)\n",
        "    #document=f: This argument provides the document to be analyzed. f is a file handle representing the opened file.\n",
        "result = poller.result()\n",
        "\n",
        "# Extract text from document\n",
        "#extracted_text = \"\\n\".join([page.content for page in result.pages])\n",
        "extracted_text=result.content\n",
        "\n",
        "# Step 2: Send extracted content to GPT for classification\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Given the following document text, analyze its content.\n",
        "Identify the underlying financial or tax-related theme, such as compliance, reporting, audit, accounting, policy, corporate finance, personal taxation, investment\n",
        "or regulatory matters.\n",
        "If there is a subcategory then make sure subcatgory is included as comma separted values in the response.\n",
        "On this analysis classify the document into the most appropriate financial or tax-related category\n",
        "that best represents its primary subject matter.\n",
        "Also return the number of pages.\n",
        "include the precide page number where Tax or related  content occurs in the documents.\n",
        "If the above exists in more than one page have it displayed as comma separated like 1,2\n",
        "Return JSON with fields: category, confidence, description, Number of Pages, Page Number.\n",
        "And make sure subcategory is included in the response.\n",
        "Document:\n",
        "{extracted_text}\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # or \"gpt-4.0\"\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d613c6d"
      },
      "source": [
        "## Chunking the document\n",
        "\n",
        "### Subtask:\n",
        "Divide the extracted text into smaller, manageable chunks. This prevents exceeding the LLM's context window and reduces the number of tokens processed per API call.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b04d24fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Divide the extracted text into smaller chunks based on a suitable chunk size, store the chunks in a list, and keep track of the starting page number for each chunk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GYxOV6k831_",
        "outputId": "d522467b-7680-4de3-bb3e-13e48ece69a5"
      },
      "source": [
        "# Determine a suitable chunk size (adjust as needed based on LLM token limits)\n",
        "# A rough estimate of characters per token is 4, so for a 4096 token limit,\n",
        "# a chunk size around 15000 characters might be a starting point.\n",
        "# However, splitting by pages or sections is often more effective for maintaining context.\n",
        "# Let's try splitting by double newlines, which often indicate paragraph breaks or section changes.\n",
        "# If this doesn't create reasonably sized chunks, we can adjust the strategy.\n",
        "\n",
        "chunks = []\n",
        "chunk_page_numbers = []\n",
        "current_chunk = \"\"\n",
        "current_page_number = 1\n",
        "# Fix: Use result.pages to get the number of pages\n",
        "characters_per_page = len(extracted_text) / len(result.pages) if len(result.pages) > 0 else 0\n",
        "\n",
        "# Split by double newlines to get potential sections\n",
        "sections = extracted_text.split('\\n\\n')\n",
        "\n",
        "# A simple approach to associate chunks with page numbers.\n",
        "# This assumes a relatively even distribution of text across pages.\n",
        "# A more accurate approach would involve analyzing the layout information from Document Intelligence result.\n",
        "char_count = 0\n",
        "for section in sections:\n",
        "    section_length = len(section) + 2 # Add 2 for the removed double newline\n",
        "    if char_count + section_length > (current_page_number * characters_per_page) and characters_per_page > 0:\n",
        "      current_page_number += 1\n",
        "\n",
        "    if len(current_chunk) + section_length > 15000: # Example chunk size limit\n",
        "        chunks.append(current_chunk)\n",
        "        chunk_page_numbers.append(current_page_number)\n",
        "        current_chunk = section\n",
        "    else:\n",
        "        current_chunk += \"\\n\\n\" + section\n",
        "\n",
        "    char_count += section_length\n",
        "\n",
        "# Add the last chunk\n",
        "if current_chunk:\n",
        "    chunks.append(current_chunk)\n",
        "    chunk_page_numbers.append(current_page_number)\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(f\"Starting page numbers for chunks: {chunk_page_numbers}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 1\n",
            "Starting page numbers for chunks: [2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "167406e7"
      },
      "source": [
        "## Summarization of chunks\n",
        "\n",
        "### Subtask:\n",
        "Use the LLM to summarize each chunk. This distills the key information from each section, reducing the overall amount of text that needs to be considered for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65038fdd"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize an empty list to store summaries and iterate through the chunks to generate summaries using the LLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e4d9636"
      },
      "source": [
        "summaries = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    prompt = f\"\"\"\n",
        "    Summarize the following document chunk, focusing on financial or tax-related information if present.\n",
        "    If there is no significant financial or tax content, provide a brief general summary of the section.\n",
        "    Make sure to keep the summary concise.\n",
        "\n",
        "    Document Chunk:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    summary = response.choices[0].message.content\n",
        "    summaries.append(summary)\n",
        "\n",
        "print(f\"Generated {len(summaries)} summaries.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce0ac9fd"
      },
      "source": [
        "relevant_chunk_indices = []\n",
        "\n",
        "for i, summary in enumerate(summaries):\n",
        "    prompt = f\"\"\"\n",
        "    Does the following summary contain significant financial or tax-related information relevant to classifying the document as financial or tax-related?\n",
        "    Respond with only 'yes' or 'no'.\n",
        "\n",
        "    Summary:\n",
        "    {summary}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=5 # Keep the response short\n",
        "    )\n",
        "    answer = response.choices[0].message.content.strip().lower()\n",
        "\n",
        "    if \"yes\" in answer:\n",
        "        relevant_chunk_indices.append(i)\n",
        "\n",
        "print(f\"Indices of relevant summaries: {relevant_chunk_indices}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bb9566"
      },
      "source": [
        "## Final classification with relevant summaries\n",
        "\n",
        "### Subtask:\n",
        "Send only the summaries of the relevant chunks, along with the original prompt, to the LLM for the final classification. This significantly reduces the total token usage compared to sending the entire document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39a1f72c"
      },
      "source": [
        "**Reasoning**:\n",
        "Construct the final prompt using the relevant summaries and call the LLM for classification, then parse the JSON response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a956ec02"
      },
      "source": [
        "# Create a list of relevant summaries\n",
        "relevant_summaries = [summaries[i] for i in relevant_chunk_indices]\n",
        "\n",
        "# Join the relevant summaries into a single string\n",
        "summaries_string = \"---\\n\".join(relevant_summaries)\n",
        "\n",
        "# Construct a new prompt for the LLM\n",
        "final_prompt = f\"\"\"\n",
        "Given the following summaries of relevant sections from a document, analyze their content.\n",
        "Identify the underlying financial or tax-related theme, such as compliance, reporting, audit, accounting, policy, corporate finance, personal taxation, investment\n",
        "or regulatory matters.\n",
        "If there is a subcategory then make sure subcatgory is included as comma separted values in the response.\n",
        "On this analysis classify the document into the most appropriate financial or tax-related category\n",
        "that best represents its primary subject matter.\n",
        "Also return the number of pages, which is {len(result.pages)}.\n",
        "include the precise page number where Tax or related content occurs in the documents.\n",
        "If the above exists in more than one page have it displayed as comma separated like 1,2\n",
        "Return JSON with fields: category, confidence, description, Number of Pages, Page Number, and subcategory.\n",
        "\n",
        "Relevant Summaries:\n",
        "{summaries_string}\n",
        "\"\"\"\n",
        "\n",
        "# Call the OpenAI API with the new prompt\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[{\"role\": \"user\", \"content\": final_prompt}]\n",
        ")\n",
        "\n",
        "# Parse the JSON response from the LLM\n",
        "import json\n",
        "classification_result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "print(json.dumps(classification_result, indent=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40adba26"
      },
      "source": [
        "import re\n",
        "\n",
        "# Extract the JSON string from the raw LLM response\n",
        "raw_response_content = response.choices[0].message.content\n",
        "json_match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', raw_response_content)\n",
        "\n",
        "classification_result = {}\n",
        "\n",
        "if json_match:\n",
        "    json_string = json_match.group(1)\n",
        "    try:\n",
        "        # Parse the extracted JSON string\n",
        "        classification_result = json.loads(json_string)\n",
        "        print(\"\\nParsed JSON result:\")\n",
        "        print(json.dumps(classification_result, indent=2))\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"\\nFailed to decode extracted JSON string: {e}\")\n",
        "        # Handle cases where the extracted string is still not valid JSON\n",
        "else:\n",
        "    print(\"\\nNo JSON block found in the LLM response.\")\n",
        "    # Handle cases where the LLM did not provide a JSON block in the expected format\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbe89a97"
      },
      "source": [
        "## Extracting page numbers\n",
        "\n",
        "### Subtask:\n",
        "Since the original document is chunked and summarized, the page numbers of the tax-related content will need to be tracked during the chunking process and potentially re-extracted or referenced based on the identified relevant chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8627166b"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize an empty list to store relevant page numbers, iterate through the relevant chunk indices, get the corresponding starting page number from the chunk_page_numbers list, append it to the relevant_page_numbers list, convert the list to a comma-separated string, and update the classification_result dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "791d27fa"
      },
      "source": [
        "relevant_page_numbers = []\n",
        "\n",
        "for i in relevant_chunk_indices:\n",
        "    relevant_page_numbers.append(chunk_page_numbers[i])\n",
        "\n",
        "page_numbers_str = \",\".join(map(str, relevant_page_numbers))\n",
        "\n",
        "classification_result['Page Number'] = page_numbers_str\n",
        "\n",
        "print(classification_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ff79a0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The document text was successfully divided into smaller chunks based on double newlines and a maximum size limit, with each chunk associated with its starting page number.\n",
        "*   An LLM (gpt-4o) was used to generate concise summaries for each chunk, prioritizing financial or tax-related content.\n",
        "*   The LLM was then used to identify which of these summaries contained significant financial or tax-related information relevant to classification.\n",
        "*   The final classification was performed by sending only the relevant summaries to the LLM, significantly reducing token usage.\n",
        "*   The LLM's response, which included the classification result in JSON format, was extracted from a markdown code block within the raw output.\n",
        "*   The starting page numbers corresponding to the identified relevant chunks were successfully extracted and included in the final classification result.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Consider refining the chunking strategy to be more robust, potentially using Document Intelligence layout information for more accurate page or section breaks, especially for complex document structures.\n",
        "*   Explore alternative methods for identifying relevant chunks that might be faster or less token-intensive than using the LLM for a simple yes/no decision on each summary.\n"
      ]
    }
  ]
}