{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cnLdAX7F6-DZWlenXvfjzoTBSJmei02B",
      "authorship_tag": "ABX9TyPPa6FZoXUksyky2XjSCdfk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/ML/blob/main/Document_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nt9UWLLRS3i"
      },
      "outputs": [],
      "source": [
        "%pip install azure-ai-formrecognizer openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Parse document using Azure Document Intelligence\n",
        "from azure.ai.formrecognizer.aio import DocumentAnalysisClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from openai import AsyncOpenAI\n",
        "import asyncio\n",
        "\n",
        "# Replace with your actual access token\n",
        "from google.colab import userdata\n",
        "DOCUMENTINTEL_KEY = userdata.get('DOCUMENTINTEL_KEY')\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "directory_path = \"/content/drive/MyDrive/ML/Training\"\n",
        "all_entries = os.listdir(directory_path)\n",
        "document_files = [entry for entry in all_entries if os.path.isfile(os.path.join(directory_path, entry))]\n",
        "\n",
        "print(document_files)\n",
        "\n",
        "# Azure Document Intelligence setup\n",
        "endpoint = \"https://documentsclassifier.cognitiveservices.azure.com/\"\n",
        "key = DOCUMENTINTEL_KEY\n",
        "\n",
        "async def process_document(file_name, document_analysis_client, openai_client):\n",
        "    \"\"\"Processes a single document using Azure Document Intelligence and OpenAI asynchronously.\"\"\"\n",
        "    file_path = os.path.join(directory_path, file_name)\n",
        "    if file_name.lower().endswith('.odt'):\n",
        "        print(f\"Skipping .odt file: {file_name}\")\n",
        "        return file_name, f\"Skipped: .odt file\"\n",
        "\n",
        "    try:\n",
        "        async with asyncio.Lock(): # Use a lock if file reading needs to be synchronized, though often not needed for reads\n",
        "             with open(file_path, \"rb\") as f:\n",
        "                # Azure Document Intelligence client for async operations\n",
        "                poller = await document_analysis_client.begin_analyze_document(\"prebuilt-document\", f)\n",
        "                result = await poller.result()\n",
        "\n",
        "        # Extract content\n",
        "        content = result.content\n",
        "\n",
        "        # Step 2: Send content to GPT for classification\n",
        "        # Original prompt template - NOT MODIFIED\n",
        "        prompt = f\"\"\"\n",
        "        Classify the following document text into an appropriate category.\n",
        "        Also return the number of pages.\n",
        "        include the precide page number which you think contains the gist of the document.\n",
        "        If the above exists in more than one page have it displayed as comma separated like 1,2\n",
        "        Number of Pages should include the over all count of the document\n",
        "        Return JSON with fields: File Name, Category, Confidence, Description, Number of Pages, Gist Page Number.\n",
        "\n",
        "        Document:\n",
        "        {content}\n",
        "        \"\"\"\n",
        "\n",
        "        response = await openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a document classifier. Classify the following document content.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt} # Using the original prompt template\n",
        "            ]\n",
        "        )\n",
        "        classification = response.choices[0].message.content\n",
        "        return file_name, classification\n",
        "\n",
        "    except Exception as e:\n",
        "        return file_name, f\"Error: {e}\"\n",
        "\n",
        "async def main():\n",
        "    document_analysis_client = DocumentAnalysisClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(DOCUMENTINTEL_KEY)\n",
        "    )\n",
        "    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    tasks = []\n",
        "    for file_name in document_files:\n",
        "        tasks.append(process_document(file_name, document_analysis_client, openai_client))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    classification_results = {}\n",
        "    for file_name, result in results:\n",
        "        classification_results[file_name] = result\n",
        "\n",
        "    # Close the clients (important for async clients)\n",
        "    await document_analysis_client.close()\n",
        "    await openai_client.close()\n",
        "\n",
        "    # Display results in JSON format\n",
        "    print(\"Classification Results (JSON):\")\n",
        "    print(json.dumps(classification_results, indent=4))\n",
        "\n",
        "# Run the async main function\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "KmsypD4PRUTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c44ca5a7"
      },
      "source": [
        "# Task\n",
        "Loop through all documents in \"/content/drive/MyDrive/ML/Training\", extract content using appropriate libraries for different file types (.csv, .xlsx, .docx, .pptx) and Azure Document Intelligence for others, send the extracted content to GPT for classification, and display the results in a beautified JSON format. Skip .odt files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3594e0b9"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install libraries required for reading different file formats (e.g., pandas for CSV/Excel, python-docx for Word, python-pptx for PowerPoint, and potentially a library for handling PDFs if not already covered by Azure).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d38202d6"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing several libraries to handle different file formats. I will use pip to install `pandas`, `python-docx`, and `python-pptx`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acf16db5"
      },
      "source": [
        "%pip install pandas python-docx python-pptx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d38041f4"
      },
      "source": [
        "## Create modular functions\n",
        "\n",
        "### Subtask:\n",
        "Create modular functions for processing different file types (.csv, .xlsx, .docx, .pptx) using appropriate libraries and a function for other file types using Azure Document Intelligence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "132ce9b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the modular functions for processing different file types and adapt the existing Azure Document Intelligence function as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0b42906"
      },
      "source": [
        "import pandas as pd\n",
        "from docx import Document\n",
        "from pptx import Presentation\n",
        "from azure.ai.formrecognizer.aio import DocumentAnalysisClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from openai import AsyncOpenAI\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "# Replace with your actual access token\n",
        "from google.colab import userdata\n",
        "DOCUMENTINTEL_KEY = userdata.get('DOCUMENTINTEL_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "directory_path = \"/content/drive/MyDrive/ML/Training\"\n",
        "\n",
        "def process_excel(file_path):\n",
        "    \"\"\"Reads an Excel file and extracts content as text.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        return df.to_string()\n",
        "    except Exception as e:\n",
        "        return f\"Error processing Excel file: {e}\"\n",
        "\n",
        "def process_csv(file_path):\n",
        "    \"\"\"Reads a CSV file and extracts content as text.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df.to_string()\n",
        "    except Exception as e:\n",
        "        return f\"Error processing CSV file: {e}\"\n",
        "\n",
        "def process_word(file_path):\n",
        "    \"\"\"Reads a Word file and extracts content as text.\"\"\"\n",
        "    try:\n",
        "        document = Document(file_path)\n",
        "        content = \"\"\n",
        "        for para in document.paragraphs:\n",
        "            content += para.text + \"\\n\"\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        return f\"Error processing Word file: {e}\"\n",
        "\n",
        "def process_powerpoint(file_path):\n",
        "    \"\"\"Reads a PowerPoint file and extracts content as text.\"\"\"\n",
        "    try:\n",
        "        prs = Presentation(file_path)\n",
        "        content = \"\"\n",
        "        for slide in prs.slides:\n",
        "            for shape in slide.shapes:\n",
        "                if hasattr(shape, \"text\"):\n",
        "                    content += shape.text + \"\\n\"\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        return f\"Error processing PowerPoint file: {e}\"\n",
        "\n",
        "async def process_document_intelligence(file_path, document_analysis_client, openai_client):\n",
        "    \"\"\"Processes a document using Azure Document Intelligence and OpenAI asynchronously.\"\"\"\n",
        "    try:\n",
        "        async with asyncio.Lock():\n",
        "             with open(file_path, \"rb\") as f:\n",
        "                poller = await document_analysis_client.begin_analyze_document(\"prebuilt-document\", f)\n",
        "                result = await poller.result()\n",
        "        content = result.content\n",
        "\n",
        "        # Step 2: Send content to GPT for classification\n",
        "        prompt = f\"\"\"\n",
        "        Classify the following document text into an appropriate category.\n",
        "        Also return the number of pages.\n",
        "        include the precide page number which you think contains the gist of the document.\n",
        "        If the above exists in more than one page have it displayed as comma separated like 1,2\n",
        "        Number of Pages should include the over all count of the document\n",
        "        Return JSON with fields: File Name, Category, Confidence, Description, Number of Pages, Gist Page Number.\n",
        "\n",
        "        Document:\n",
        "        {content}\n",
        "        \"\"\"\n",
        "\n",
        "        response = await openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a document classifier. Classify the following document content.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        classification = response.choices[0].message.content\n",
        "        return os.path.basename(file_path), classification\n",
        "\n",
        "    except Exception as e:\n",
        "        return os.path.basename(file_path), f\"Error: {e}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917e699a"
      },
      "source": [
        "## Update main processing logic\n",
        "\n",
        "### Subtask:\n",
        "Modify the main loop to identify the file type based on its extension and call the appropriate processing function for each file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30993aed"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the main loop to iterate through files, determine their type, and call the appropriate processing function, including handling skipped .odt files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32918d5c"
      },
      "source": [
        "async def main():\n",
        "    document_analysis_client = DocumentAnalysisClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(DOCUMENTINTEL_KEY)\n",
        "    )\n",
        "    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    classification_results = {}\n",
        "    tasks = []\n",
        "\n",
        "    for file_name in document_files:\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        file_extension = os.path.splitext(file_name)[1].lower()\n",
        "\n",
        "        if file_extension == '.odt':\n",
        "            print(f\"Skipping .odt file: {file_name}\")\n",
        "            classification_results[file_name] = \"Skipped: .odt file\"\n",
        "        elif file_extension == '.csv':\n",
        "            content = process_csv(file_path)\n",
        "            # For simplicity, directly classify content from local files\n",
        "            prompt = f\"\"\"\n",
        "            Classify the following document text into an appropriate category.\n",
        "            Also return the number of pages.\n",
        "            include the precide page number which you think contains the gist of the document.\n",
        "            If the above exists in more than one page have it displayed as comma separated like 1,2\n",
        "            Number of Pages should include the over all count of the document\n",
        "            Return JSON with fields: File Name, Category, Confidence, Description, Number of Pages, Gist Page Number.\n",
        "\n",
        "            Document:\n",
        "            {content}\n",
        "            \"\"\"\n",
        "            tasks.append(openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a document classifier. Classify the following document content.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            ))\n",
        "            classification_results[file_name] = \"Processing with OpenAI\" # Placeholder\n",
        "        elif file_extension == '.xls' or file_extension == '.xlsx':\n",
        "            content = process_excel(file_path)\n",
        "            prompt = f\"\"\"\n",
        "            Classify the following document text into an appropriate category.\n",
        "            Also return the number of pages.\n",
        "            include the precide page number which you think contains the gist of the document.\n",
        "            If the above exists in more than one page have it displayed as comma separated like 1,2\n",
        "            Number of Pages should include the over all count of the document\n",
        "            Return JSON with fields: File Name, Category, Confidence, Description, Number of Pages, Gist Page Number.\n",
        "\n",
        "            Document:\n",
        "            {content}\n",
        "            \"\"\"\n",
        "            tasks.append(openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a document classifier. Classify the following document content.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            ))\n",
        "            classification_results[file_name] = \"Processing with OpenAI\" # Placeholder\n",
        "        elif file_extension == '.docx':\n",
        "            content = process_word(file_path)\n",
        "            prompt = f\"\"\"\n",
        "            Classify the following document text into an appropriate category.\n",
        "            Also return the number of pages.\n",
        "            include the precide page number which you think contains the gist of the document.\n",
        "            If the above exists in more than one page have it displayed as comma separated like 1,2\n",
        "            Number of Pages should include the over all count of the document\n",
        "            Return JSON with fields: File Name, Category, Confidence, Description, Number of Pages, Gist Page Number.\n",
        "\n",
        "            Document:\n",
        "            {content}\n",
        "            \"\"\"\n",
        "            tasks.append(openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a document classifier. Classify the following document content.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            ))\n",
        "            classification_results[file_name] = \"Processing with OpenAI\" # Placeholder\n",
        "        elif file_extension == '.pptx':\n",
        "            content = process_powerpoint(file_path)\n",
        "            prompt = f\"\"\"\n",
        "            Classify the following document text into an appropriate category.\n",
        "            Also return the number of pages.\n",
        "            include the precide page number which you think contains the gist of the document.\n",
        "            If the above exists in more than one page have it displayed as comma separated like 1,2\n",
        "            Number of Pages should include the over all count of the document\n",
        "            Return JSON with fields: File Name, Category, Confidence, Description, Number of Pages, Gist Page Number.\n",
        "\n",
        "            Document:\n",
        "            {content}\n",
        "            \"\"\"\n",
        "            tasks.append(openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a document classifier. Classify the following document content.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            ))\n",
        "            classification_results[file_name] = \"Processing with OpenAI\" # Placeholder\n",
        "        else:\n",
        "            # Use Azure Document Intelligence for other types (including PDFs and images)\n",
        "            tasks.append(process_document_intelligence(file_path, document_analysis_client, openai_client))\n",
        "\n",
        "    # Wait for all OpenAI tasks to complete\n",
        "    openai_responses = await asyncio.gather(*[task for task in tasks if \"Processing with OpenAI\" in classification_results.values()])\n",
        "\n",
        "    # Update results with OpenAI responses\n",
        "    openai_response_index = 0\n",
        "    for file_name in document_files:\n",
        "        if classification_results.get(file_name) == \"Processing with OpenAI\":\n",
        "            classification_results[file_name] = openai_responses[openai_response_index].choices[0].message.content\n",
        "            openai_response_index += 1\n",
        "        elif classification_results.get(file_name) != \"Skipped: .odt file\":\n",
        "             # Handle results from process_document_intelligence\n",
        "             pass # This part will be handled by the original async gather if needed\n",
        "\n",
        "    # Gather results from process_document_intelligence (if any)\n",
        "    di_tasks_results = await asyncio.gather(*[task for task in tasks if not isinstance(task, asyncio.Task)])\n",
        "    for file_name, result in di_tasks_results:\n",
        "         classification_results[file_name] = result\n",
        "\n",
        "\n",
        "    # Close the clients\n",
        "    await document_analysis_client.close()\n",
        "    await openai_client.close()\n",
        "\n",
        "    # Display results in JSON format\n",
        "    print(\"Classification Results (JSON):\")\n",
        "    print(json.dumps(classification_results, indent=4))\n",
        "\n",
        "# Run the async main function\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}