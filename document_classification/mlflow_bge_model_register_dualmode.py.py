# Databricks notebook source
# Install required libraries for the BAAI/bge-m3 model
%pip install transformers accelerate bitsandbytes mlflow torch torchvision

# Define the model identifier to be used throughout the notebook
model_name = "BAAI/bge-m3"

# Restart the Python runtime to ensure the new packages are loaded
dbutils.library.restartPython()

# COMMAND ----------

import os
from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig
import mlflow
import mlflow.transformers

# COMMAND ----------

import os
import json
import torch
import mlflow
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
from typing import Any, Dict

# ============================================================
# Dual-Mode MLflow Model â€” Embedding + Classification
# ============================================================
class DualModeTaxModel(mlflow.pyfunc.PythonModel):
    """
    This custom MLflow model can serve in two modes:
      â€¢ mode="embedding" â†’ returns vector embeddings
      â€¢ mode="classification" â†’ returns logits for tax labels
    """

    def load_context(self, context):
        """
        Load model artifacts from the MLflow context.
        Disables safetensors to avoid 'header too large' issues.
        """
        # ðŸ§© Ensure safe deserialization
        os.environ["SAFETENSORS_FAST_GPU"] = "0"
        os.environ["SAFETENSORS_DISABLE"] = "1"

        model_path = context.artifacts["model_dir"]

        # Load tokenizer and both model heads (safetensors disabled)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            trust_remote_code=True,
            ignore_mismatched_sizes=True,
            safe_serialization=False  # forces PyTorch .bin loading
        )
        self.embedder = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            safe_serialization=False  # disables safetensors
        )

        self.model.eval()
        self.embedder.eval()

    def predict(self, context, model_input):
    #def predict(self, context: mlflow.pyfunc.model.PythonModelContext, model_input: Any) -> Dict[str, Any]:

        """
        Handles both:
          - {"inputs": [...], "mode": "embedding" | "classification"}
          - [{"inputs": [...], "mode": "..."}] (Databricks Serving wrapper)
        """
        import json
        import torch

        # Handle Databricks Serving input structure
        if isinstance(model_input, list):
            model_input = model_input[0]
        elif isinstance(model_input, str):
            model_input = json.loads(model_input)

        inputs = model_input.get("inputs", [])
        mode = model_input.get("mode", "classification")

        # EMBEDDING MODE
        if mode == "embedding":
            with torch.no_grad():
                tokens = self.tokenizer(
                    inputs,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                )
                emb = self.embedder(**tokens).last_hidden_state[:, 0, :].numpy()
            return {"embeddings": emb.tolist()}

        # CLASSIFICATION MODE
        elif mode == "classification":
            with torch.no_grad():
                tokens = self.tokenizer(
                    inputs,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                )
                logits = self.model(**tokens).logits.numpy()
            return {"predictions": [{"logits": log.tolist()} for log in logits]}

        else:
            raise ValueError("mode must be either 'embedding' or 'classification'")


# ============================================================
# Local test entry point (optional)
# ============================================================
if __name__ == "__main__":
    # path to your locally fine-tuned model
    local_path = "/dbfs/tmp/BAAI/bge-m3-v2"

    # Disable safetensors globally before loading
    os.environ["SAFETENSORS_DISABLE"] = "1"

    model = DualModeTaxModel()
    try:
        model.tokenizer = AutoTokenizer.from_pretrained(local_path)
        model.model = AutoModelForSequenceClassification.from_pretrained(local_path)
        model.embedder = AutoModel.from_pretrained(local_path)
    except Exception as e:
        print(f"Warning: model partially loaded â€” {e}")   


# COMMAND ----------

# simple test
sample = {
    "inputs": [
        "Î— ÎºÏÏÎ¹Î± Ï€Î·Î³Î® ÎµÎ¹ÏƒÎ¿Î´Î®Î¼Î±Ï„Î¿Ï‚ Ï„Î·Ï‚ Î‘Ï…Ï„Î¿ÎºÏÎ±Ï„Î¿ÏÎ¯Î±Ï‚ Ï„Ï‰Î½ ÎœÎ¿Ï…Î³Î¬Î» Î®Ï„Î±Î½ Ï„Î± Î­ÏƒÎ¿Î´Î± Î±Ï€ÏŒ Ï„Î· Î³Î· (mal). Î¤Î¿ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿ Î¼Î­ÏÎ¿Ï‚ Ï„Î¿Ï… Ï€Î»Î·Î¸Ï…ÏƒÎ¼Î¿Ï Î®Ï„Î±Î½ Î±Î³ÏÏŒÏ„ÎµÏ‚, ÎµÏ€Î¿Î¼Î­Î½Ï‰Ï‚ Î· Î³ÎµÏ‰ÏÎ³Î¯Î± Ï†Î¿ÏÎ¿Î»Î¿Î³Î¿ÏÎ½Ï„Î±Î½ Î²Î±ÏÎ¹Î¬, Î±Î»Î»Î¬ Î¼Îµ Î­Î½Î±Î½ Î¿ÏÎ³Î±Î½Ï‰Î¼Î­Î½Î¿ Ï„ÏÏŒÏ€Î¿. Î— Î³Î· ÎµÎ¾ÎµÏ„Î±Î¶ÏŒÏ„Î±Î½ ÎºÎ±Î¹ Ï„Î±Î¾Î¹Î½Î¿Î¼Î¿ÏÎ½Ï„Î±Î½ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î· Î³Î¿Î½Î¹Î¼ÏŒÏ„Î·Ï„Î±, Ï„Î¿Î½ Ï„ÏÏ€Î¿ Ï„Î·Ï‚ ÎºÎ±Î»Î»Î¹Î­ÏÎ³ÎµÎ¹Î±Ï‚ ÎºÎ±Î¹ Ï„Î·Î½ Ï€Î±ÏÎ±Î³Ï‰Î³Î¹ÎºÏŒÏ„Î·Ï„Î±. ÎŸÎ¹ Ï†ÏŒÏÎ¿Î¹ Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶Î¿Î½Ï„Î±Î½ Î±Î½Î¬Î»Î¿Î³Î± â€” ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ Ï„Î¿ Î­Î½Î± Ï„ÏÎ¯Ï„Î¿ Ï„Î·Ï‚ Î¼Î­ÏƒÎ·Ï‚ Î±Î¾Î¯Î±Ï‚ Ï„Î·Ï‚ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚. Î— Ï€Î»Î·ÏÏ‰Î¼Î® Î¼Ï€Î¿ÏÎ¿ÏÏƒÎµ Î½Î± Î³Î¯Î½ÎµÎ¹ ÏƒÎµ Î¼ÎµÏ„ÏÎ·Ï„Î¬ Î® ÏƒÎµ ÎµÎ¯Î´Î¿Ï‚ (ÏƒÎ¹Ï„Î·ÏÎ¬ Î® ÎºÎ±Î»Î»Î¹Î­ÏÎ³ÎµÎ¹ÎµÏ‚), Î±Î½ ÎºÎ±Î¹ ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÏ€Î¿Ï‡Î® Ï„Î¿Ï… Î£Î±Ï‡ Î¤Î¶Î±Ï‡Î¬Î½, Î¿Î¹ Ï€Î»Î·ÏÏ‰Î¼Î­Ï‚ ÏƒÎµ Î¼ÎµÏ„ÏÎ·Ï„Î¬ Î®Ï„Î±Î½ Ï€Î¹Î¿ ÏƒÏ…Î½Î·Î¸Î¹ÏƒÎ¼Î­Î½ÎµÏ‚ Î»ÏŒÎ³Ï‰ Ï„Î·Ï‚ Î½Î¿Î¼Î¹ÏƒÎ¼Î±Ï„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Ï„Î·Ï‚ Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¯Î±Ï‚."
    ], 
    "mode": "classification"
}
print(model.predict(None, sample))

# COMMAND ----------

import mlflow
mlflow.set_registry_uri("databricks")   # Legacy registry


# COMMAND ----------

mlflow.pyfunc.log_model(
    artifact_path="dual_mode_tax_model",
    python_model=DualModeTaxModel(),
    artifacts={"model_dir": local_path},
    registered_model_name="bge-m3_base_dualmode",
    pip_requirements=[
        # Core libraries pinned for cross-compatibility
        "transformers==4.42.0",        # supports bge-m3 + XLMRoberta, no init conflict
        "torch>=2.1.0,<2.4.0",         # ensure CUDA kernels are stable
        "mlflow>=3.5.1",
        "pandas>=2.2.0,<3.0.0",
        "numpy>=1.26.0,<2.0.0",
        "accelerate>=0.28.0",
        "sentencepiece>=0.1.99",
        "pyopenssl>=24.1.0",
        "cloudpickle==3.0.0",
        "cryptography>=43,<47"
    ]
)
