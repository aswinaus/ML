{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZwM1YUH5zQhI8sdVLHuUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/ML/blob/main/ADLS_Databricks_ApacheSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fr7AxrT5T_r"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, IndexToString\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat_ws, col\n",
        "\n",
        "\n",
        "# Initialize SparkSession for Databricks\n",
        "# In Databricks, the SparkSession is typically pre-configured,\n",
        "# so you don't need to explicitly set Hadoop Azure dependencies.\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"TF-IDF-Document-Classification\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Load millions of docs into Spark DataFrame: (id, text, label)\n",
        "# Update the path format for Databricks file system access if necessary.\n",
        "# For example, if your data is in DBFS, the path might look like \"/mnt/docs/documents.json\"\n",
        "# or if it's in cloud storage configured with Databricks, the path format might vary.\n",
        "# Replace \"abfss://docs@mydatalake.dfs.core.windows.net/documents.json\"\n",
        "# with the correct path for your Databricks environment.\n",
        "#df = spark.read.json(\"/mnt/raw/documents.json\")\n",
        "\n",
        "#df = spark.read.option(\"header\", \"true\").csv(\"abfss://docsclasscontainer@docsclassstorageaccount.dfs.core.windows.net/tax_statistics_dataset - Copy - Copy.csv\")\n",
        "#df.display()\n",
        "\n",
        "\n",
        "configs = {\n",
        "  \"fs.azure.account.auth.type\": \"OAuth\",\n",
        "  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
        "  \"fs.azure.account.oauth2.client.id\": \"71127fdf-6f8b-4a6d-942a-40f8c9a6e754\",\n",
        "  \"fs.azure.account.oauth2.client.secret\": \"e4l8Q~wsV2oa58YkHLtjnhq-V.AWcwftWH1kAckV\",\n",
        "  \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/115ee48c-5146-4054-9f33-83e2bfe089fd/oauth2/token\"\n",
        "}\n",
        "\n",
        "\n",
        "spark.conf.set(\"fs.azure.account.auth.type.docsclassstorageaccount.dfs.core.windows.net\", \"OAuth\")\n",
        "for key, val in configs.items():\n",
        "    spark.conf.set(f\"{key}.docsclassstorageaccount.dfs.core.windows.net\", val)\n",
        "\n",
        "\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"abfss://docsclasscontainer@docsclassstorageaccount.dfs.core.windows.net/tax_statistics_dataset - Copy - Copy.csv\")\n",
        "# Rename columns to remove invalid characters\n",
        "for column_name in df.columns:\n",
        "    new_col = column_name.replace(' ', '_').replace(',', '').replace(';', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\\n', '').replace('\\t', '').replace('=', '')\n",
        "    df = df.withColumnRenamed(column_name, new_col)\n",
        "\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"tax_statistics_dataset\")\n",
        "\n",
        "df = spark.read.table(\"tax_statistics_dataset\")\n",
        "display(df)\n",
        "\n",
        "# Combine text columns\n",
        "df = df.withColumn(\"combined_text\", concat_ws(\" \", col(\"Size_of_adjusted_gross_income\"), col(\"STATE\")))\n",
        "\n",
        "df.printSchema()\n",
        "display(df.select(\"Size_of_adjusted_gross_income\", \"STATE\", \"combined_text\").limit(10))\n",
        "\n",
        "# Create label column by combining 'Size_of_adjusted_gross_income' and 'STATE' and indexing it\n",
        "label_indexer = StringIndexer(inputCol=\"combined_text\", outputCol=\"indexedLabel\").fit(df)\n",
        "\n",
        "# Tokenize\n",
        "# It takes the raw text from the inputCol (which is \"combined_text\" in this case) and splits it into individual words or tokens.\n",
        "# inputCol=\"combined_text\": Specifies the column in the DataFrame that contains the text need to tokenize.\n",
        "# outputCol=\"words\": Specifies the name of the new column that will be added to the DataFrame.\n",
        "# This column will contain an array of the individual words extracted from the combined_text.\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"combined_text\", outputCol=\"words\")\n",
        "\n",
        "# Term Frequency step\n",
        "# This step converts the collection of words (the output of the Tokenizer from the previous step) into numerical feature vectors.\n",
        "# It uses a hashing trick to map words to indices in a fixed-size vector.\n",
        "\n",
        "# inputCol=\"words\": Takes the array of words from the previous step as input.\n",
        "\n",
        "# outputCol=\"rawFeatures\": Creates a new column containing the raw term frequency vectors.\n",
        "# Each element in the vector represents the count of a specific hashed word in the document.\n",
        "\n",
        "# numFeatures=100000: This is the size of the feature vector. A larger number can reduce collisions in the hashing process but also increases memory usage.\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "\n",
        "# Inverse Document Frequency step This step re-weights the raw term frequency features.\n",
        "# The idea is to give less importance to words that appear frequently across many documents (like \"the\", \"a\", \"is\") and more importance to words\n",
        "# that are unique to specific documents.\n",
        "\n",
        "# inputCol=\"rawFeatures\": Takes the raw term frequency vectors from the HashingTF step as input.\n",
        "\n",
        "# outputCol=\"features\": Creates a new column containing the final TF-IDF feature vectors.\n",
        "# These vectors are a more refined representation of the text data where the importance of each word is weighted by its frequency\n",
        "# in the document and its rarity across the entire dataset.\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "\n",
        "# Together, these three steps transform your raw text data into a numerical format (features) that can be used by a machine learning model like Logistic Regression for tasks like document classification.\n",
        "# This process is a common technique in natural language processing for preparing text data for machine learning algorithms.\n",
        "# Logistic Regression\n",
        "# Logistic Regression: This model is a linear model that predicts the probability of a binary outcome.\n",
        "# In this case, it's trained on the TF-IDF features extracted from the text to classify documents into predefined categories. It relies on statistical relationships between features and labels.\n",
        "\n",
        "# maxIter=20: This parameter sets the maximum number of iterations the optimization algorithm will\n",
        "# run to find the model's coefficients. A higher number of iterations might lead to a better fit but can also increase training time.\n",
        "\n",
        "# regParam=0.01: This parameter controls the amount of regularization applied to the model.\n",
        "# Regularization helps prevent overfitting by adding a penalty to the model's complexity.\n",
        "# A value of 0.01 indicates a small amount of regularization.\n",
        "\n",
        "# labelCol=\"indexedLabel\": This parameter specifies the name of the column in the DataFrame that contains the target variable or labels.\n",
        "# In this case, it's set to \"indexedLabel\", which is the numerical column created by the StringIndexer earlier in the pipeline.\n",
        "# The model will learn to predict these labels based on the input features.\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.01, labelCol=\"indexedLabel\")\n",
        "\n",
        "# Convert indexed labels back to original labels for predictions\n",
        "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=label_indexer.labels)\n",
        "\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[label_indexer, tokenizer, hashingTF, idf, lr, label_converter])\n",
        "\n",
        "# Train model\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "# Predict\n",
        "predictions = model.transform(df)\n",
        "\n",
        "# Display predictions\n",
        "display(predictions.select(\"Size_of_adjusted_gross_income\", \"STATE\", \"combined_text\", \"indexedLabel\", \"prediction\", \"predictedLabel\").limit(20))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fed712c3"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Replace with the actual content read from your document\n",
        "document_content = \"\"\"\n",
        "This is the content of your document.\n",
        "It can have multiple lines and paragraphs.\n",
        "You would read this from '/content/Transfer Priciing Application.txt'\n",
        "\n",
        "\n",
        "How to define a transfer pricing methodology?\n",
        "\n",
        "Transfer pricing methodology is an  approach to determining the transfer price.\n",
        "\n",
        "Let me give you an example.\n",
        "Example\n",
        "Imagine the Vista Group - a multinational company  that manufactures and distributes widgets. There\n",
        "are three key business processes:\n",
        "manufacturing,\n",
        "distribution and\n",
        "sale to the final customer.\n",
        "\n",
        "Usually, modern business involves the  registration of separate companies,\n",
        "or entities, that form a group of companies.\n",
        "\n",
        "It is often the case that these companies  are located in different countries, and local\n",
        "authorities control the tax compliance of these  entities. In essence, tax authorities want to\n",
        "ensure that the business profits are allocated to  the parts of the supply chain that generate value.\n",
        "\n",
        "One of the most critical aspects  of international tax compliance is transfer pricing - the process of  establishing prices in intercompany transactions.\n",
        "\n",
        "In our example, there are two  intercompany transactions -\n",
        "The manufacturing company sells  widgets to the Distribution company,\n",
        "and the Distribution company  sells goods to a sales company.\n",
        "\n",
        "In both transactions, Vista  Group needs to establish pricing methodologies that comply  with transfer pricing regulations.\n",
        "\n",
        "For example, the manufacturing entity can  price goods based on the manufacturing cost and a mark-up, and\n",
        "             the distribution company will  determine prices on a resale margin basis.\n",
        "\n",
        "That is what we call transfer pricing methodologies  - the approaches to setting the transfer price.\n",
        "\n",
        "In real life, multinational companies  have lots of different transactions,\n",
        "including intragroup services, loans,  financial transactions, and asset transfers.\n",
        "\n",
        "The transfer pricing management system aims to  set, track and control transfer pricing models\n",
        "or methodologies, and ensure the consistency  and central monitoring of the transfer pricing\n",
        "process.\n",
        "\n",
        "Without a proper framework, the  control gap can grow substantially when\n",
        "the size and complexity of the company  increase. While tax managers can manage\n",
        "Two approaches to transfer pricing\n",
        "\n",
        "transfer pricing methodologies in a traditional  way using Excels, Word documents and emails,\n",
        "there is a better approach  called digital transfer pricing.\n",
        "\n",
        "Need a dedicated digital workflow that allows defining and controlling TP  methodologies in any multinational company.\n",
        "\n",
        "The process includes 8 steps:\n",
        "Defining TP models\n",
        "Specifying functions, assets and risks Adding definitions\n",
        "Selecting transfer pricing methods Mapping transactions to tp models\n",
        "Mapping TP attributes, such  as benchmarking studies,\n",
        "Identifying profit level indicators, and\n",
        "Validating arm’s length nature of the transaction\n",
        "\n",
        "Digital transfer pricing applications  are built around the workflows\n",
        "that allow defining TP methodologies  in a structured and consistent way.\n",
        "\n",
        "When definining the TP methodologies,the first step is to add the TP models.\n",
        "\n",
        "TP models are particular methodologies  applied to a set of transactions.\n",
        "then - indicate functions, assets and  risks, in other words - functional analysis\n",
        "\n",
        "Next - specify the definitions. For example, one  of the sides of the TP model may be a contract\n",
        "manufacturer, while in the other model -  a limited or fully fledged distributor.\n",
        "\n",
        "In the following step you need to  select transfer pricing methods\n",
        "and indicate appropriate characteristics, such as  tested party and the profit level indicator used.\n",
        "\n",
        "Next - map the transactions  to TP models. In this step,\n",
        "you indicate which transactions were priced  in accordance with the pre-set TP methodology.\n",
        "\n",
        "Map TP attributes, like benchmarking  studies, to transactions. By this,\n",
        "you let the application know which benchmarks  are relevant for price setting or price testing.\n",
        "\n",
        "Next, indicate profit level indicatiors.  These can be ex-ante or ex-post results.\n",
        "\n",
        "And, at the end, validate the arm’s  length nature of transactions,\n",
        "ensuring the prices are in line  with transfer pricing methodologies.\n",
        "\n",
        "Why go digital\n",
        "By using a digital transfer pricing platform, you  not just solve one problem - like preparation of\n",
        "transfer pricing documentation. Instead, you build  an end-to-end system that enables full real-time\n",
        "control over your transfer pricing, where all  applications are integrated and synchronized.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create a list of Rows, each representing a document\n",
        "# For a single document, you'll have one row.\n",
        "# You would typically determine the 'label' based on your classification task.\n",
        "# Here, we use a placeholder label.\n",
        "data = [Row(id=\"doc1\", text=document_content, label=\"unknown\")]\n",
        "\n",
        "# Create the Spark DataFrame\n",
        "sample_df = spark.createDataFrame(data)\n",
        "\n",
        "# Display the sample DataFrame\n",
        "display(sample_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}